-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  id: MarcheggianiA19
  title: Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling
  authors: Diego Marcheggiani, Ivan Titov
  doc-url: https://arxiv.org/pdf/1909.09814.pdf
  booktitle: Arxiv
  abstract: >
    Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. 
    Even though most semantic-role formalisms are built upon constituent syntax and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. 
    In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. 
    Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. 
    First, initial node representations are produced by `composing' word representations of the first and the last word in the constituent. 
    Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. 
    Finally, the constituent representations are `decomposed' back into word representations which in turn are used as input to the SRL classifier. 
    We show the effectiveness of our syntax-aware model on standard CoNLL-2005, CoNLL-2012, and FrameNet benchmarks.
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  id: Angelidis19
  title: 'Book QA: Stories of Challenges and Opportunities'
  authors: Stefanos Angelidis, Lea Frermann, Diego Marcheggiani, Roi Blanco, Lluís Màrquez
  doc-url: https://www.aclweb.org/anthology/D19-5811.pdf
  booktitle: Proceedings of the 2nd Workshop on Machine Reading for Question Answering
  venue: conference

  abstract: >
    We present a system for answering questions based on the full text of books (BookQA), which first selects book passages given a question at hand, and then uses a memory network to reason and predict an answer. 
    To improve generalization, we pretrain our memory network using artificial questions generated from book sentences. 
    We experiment with the recently published NarrativeQA corpus, on the subset of Who questions, which expect book characters as answers. 
    We experimentally show that BERT-based retrieval and pretraining improve over baseline results significantly. 
    At the same time, we confirm that NarrativeQA is a highly challenging data set, and that there is need for novel research in order to achieve high-precision BookQA results. 
    We analyze some of the bottlenecks of the current approach, and we argue that more research is needed on text representation, retrieval of relevant passages, and reasoning, including commonsense knowledge.
  bibtex: >
    @inproceedings{angelidis-etal-2019-book,
      title = "Book {QA}: Stories of Challenges and Opportunities",
      author = "Angelidis, Stefanos  and
        Frermann, Lea  and
        Marcheggiani, Diego  and
        Blanco, Roi  and
        M{\`a}rquez, Llu{\'\i}s",
      booktitle = "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
      month = nov,
      year = "2019",
      address = "Hong Kong, China",
      publisher = "Association for Computational Linguistics",
      url = "https://www.aclweb.org/anthology/D19-5811",
      doi = "10.18653/v1/D19-5811",
      pages = "78--85",
    }

-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  id: DelTredici19
  title: 'You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in NLP'
  authors: Marco Del Tredici, Diego Marcheggiani, Sabine Schulte im Walde and Raquel Fernández
  doc-url: https://www.aclweb.org/anthology/D19-1477.pdf
  booktitle: Proceedings of EMNLP-IJCNLP
  venue: conference

  abstract: >
    Information about individuals can help to better understand what they say, particularly in social media where texts are short. 
    Current approaches to modelling social media users pay attention to their social connections, but exploit this information in a static way, treating all connections uniformly. 
    This ignores the fact, well known in sociolinguistics, that an individual may be part of several communities which are not equally relevant in all communicative situations. 
    We present a model based on Graph Attention Networks that captures this observation. 
    It dynamically explores the social graph of a user, computes a user representation given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. 
    We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods.
  bibtex: >
    @inproceedings{del-tredici-etal-2019-shall,
      title = "You Shall Know a User by the Company It Keeps: Dynamic Representations for Social Media Users in {NLP}",
      author = "Del Tredici, Marco  and
        Marcheggiani, Diego  and
        Schulte im Walde, Sabine  and
        Fern{\'a}ndez, Raquel",
      booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      month = nov,
      year = "2019",
      address = "Hong Kong, China",
      publisher = "Association for Computational Linguistics",
      url = "https://www.aclweb.org/anthology/D19-1477",
      doi = "10.18653/v1/D19-1477",
      pages = "4701--4711",
    }



